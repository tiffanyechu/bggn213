---
title: "Class 09"
author: "Tiffany Chu"
date: "February 5, 2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

Let's try the `kmeans()` function in R to cluster some made-up example data.

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20
```{r}
km <- kmeans(x, centers=2, nstart = 20)
```


Inspect/print the results
```{r}
km
```

Clustering vector gives whether points are in cluster 1 or cluster 2, since we asked for 2 clusters of size 30 each

What is in the output object `km` - I can use the `attributes()` function to find this info :-)
```{r}
attributes(km)
```

Q. How many points are in each cluster?

```{r}
km$size
```

Q. What ‘component’ of your result object details
- cluster size?
- cluster assignment/membership
- cluster center?

```{r}
km$cluster
```
Let's check how many 2s and 1s are in this vector with the `table()` function.
```{r}
table(km$cluster)
```

Plot x colored by the kmeans cluster assignment

```{r}
plot(x, col=km$cluster)

#c(rep("red", 30), rep("blue", 30))
#plot(x, col=km$cluster+2)
```

Plot x colored by the kmeans cluster assignment and add cluster centers as blue points
```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```



## Hierarchical clustering in R

The `hclus()` function is the main hierarchical clustering method in R ad it **must** be passed a *distance matrix* as an input, not your raw data!

```{r}
hc <- hclust( dist(x) )
hc
```

```{r}
plot(hc)
abline(h=6, col="red", lty=2)
abline(h=3, col="blue", lty=2)
```
You can ask `cutree` for the `h` height to cut the tree.
```{r}
cutree(hc, h=6)
```

You can ask `cutree` for the `h` height to cut the tree, and use `table()` to list the number of groups 
```{r}
table( cutree(hc, h=3) )
```

You can also ask `cutree()` for the `k` number of groups that you want.
```{r}
cutree(hc, k=5)
```

Generate some made up random data that's a little more realistic and has overlap:

```{r}
x<-rbind(
matrix(rnorm(100, mean=0, sd=0.3), ncol=2), # c1
matrix(rnorm(100, mean=1, sd=0.3), ncol=2), # c2
matrix(c(rnorm(50, mean=1, sd=0.3), # c3
         rnorm(50, mean=0, sd=0.3)), ncol=2))
colnames(x) <-c("x","y")

# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col<-as.factor(rep(c("c1","c2","c3"), each=50) )
plot(x,col=col)
```

Q. Use the dist(), hclust(), plot() and cutree()
  functions to return 2 and 3 clusters

# Remember, need to pass distance matrix to `hclust()`
```{r}
hc <- hclust( dist(x))
plot(hc)
```

```{r}
grps3 <- cutree(hc, k=3)
grps3
```

```{r}
table(grps3)
```

Q. How does this compare to your known 'col' groups?
```{r}
plot(x, col=grps3)
```

Generate a cross-table:
within clusters in grps3, how many come from c1, c2, or c3?

```{r}
table(grps3, col)
```




